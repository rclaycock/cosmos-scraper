```markdown
# cosmos-scraper

This repository scrapes multiple Cosmos pages (headless Chromium via Playwright), writes one JSON per gallery into `public/`, and publishes `public/` to the `gh-pages` branch via GitHub Actions.

What is included
- .github/workflows/cosmos-scrape.yml — scheduled + manual workflow that scrapes the list of URLs and publishes public/*.json to gh-pages
- scraper/scrape.mjs — Playwright script that scrapes images and videos; accepts:
  - COSMOS_URL (env) — page to scrape
  - OUT_FILE (env) — output path (defaults to public/gallery.json)
- package.json — includes playwright dependency
- .gitignore — ignores public/ on main
- public/ — will be generated by CI and published to gh-pages

Configured URLs (in workflow)
- https://www.cosmos.so/rlphoto/swim
- https://www.cosmos.so/rlphoto/studio-tests
- https://www.cosmos.so/rlphoto/studio-test-feminine
- https://www.cosmos.so/rlphoto/swim-resort
- https://www.cosmos.so/rlphoto/location-tests

How to install and run locally
1. Clone the repo:
   - git clone git@github.com:rclaycock/cosmos-scraper.git
   - cd cosmos-scraper
2. Install dependencies:
   - npm ci
   - npx playwright install --with-deps chromium
3. Run one scrape manually (example):
   - COSMOS_URL="https://www.cosmos.so/rlphoto/swim" OUT_FILE="public/swim.json" npm run scrape

How to push these files to GitHub (if you haven’t already)
1. git add . && git commit -m "Add scraper + multi-gallery workflow"
2. git push origin main

After the workflow runs
- The Actions tab will show the workflow run. After success the gh-pages branch will be created/updated with files like:
  - https://<user>.github.io/<repo>/swim.json
  - https://<user>.github.io/<repo>/studio-tests.json
  - etc.

Enable GitHub Pages
- Settings → Pages → Source: Deploy from a branch → Branch: gh-pages / root (or follow the UI after the first workflow run; GitHub sometimes auto-selects).

If you want a single combined index JSON (all galleries in one file) or different filenames, tell me and I’ll provide the adjusted workflow and scrape script.
```